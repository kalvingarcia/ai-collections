{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from jax import numpy as jnp, random, jit, grad, pmap, vmap, tree_map, local_device_count, devices\n",
        "from jax.tools.colab_tpu import setup_tpu\n",
        "from jax.lax import fori_loop, while_loop, switch, cond, pmean\n",
        "from jax._src.lax.slicing import dynamic_slice\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from traitlets.config.configurable import deepcopy\n",
        "from collections import namedtuple\n",
        "from typing import NamedTuple\n",
        "from functools import partial\n",
        "import csv"
      ],
      "metadata": {
        "id": "xX1Guh6Jg2O7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "setup_tpu()\n",
        "n_devices = local_device_count()"
      ],
      "metadata": {
        "id": "lCPDbm0m9hhd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "seed = 0\n",
        "key = random.PRNGKey(seed)"
      ],
      "metadata": {
        "id": "cETWNjNkxPHC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CONST\n",
        "ALPHA = .3\n",
        "\n",
        "# activation functions and activation derivatives\n",
        "SIGMOID = 0\n",
        "RELU = 1\n",
        "ELU = 2\n",
        "LRELU = 3\n",
        "TANH = 4\n",
        "\n",
        "# sigmoid\n",
        "@jit\n",
        "def sigmoid(x):\n",
        "  return 1 / (1 + jnp.exp(-x))\n",
        "\n",
        "@jit\n",
        "def d_sigmoid(x):\n",
        "  f_x = sigmoid(x)\n",
        "  return f_x * (1 - f_x)\n",
        "\n",
        "# ReLU\n",
        "@jit\n",
        "def relu(x):\n",
        "  return (x > 0) * x\n",
        "@jit\n",
        "def d_relu(x):\n",
        "  return (x > 0) * 1.0\n",
        "\n",
        "# ELU\n",
        "@jit\n",
        "def elu(x):\n",
        "  return ((x > 0) * x) + ((x <= 0) * (jnp.exp(x) - 1) * ALPHA)\n",
        "\n",
        "@jit\n",
        "def d_elu(x):\n",
        "  return ((x > 0) * 1.0) + ((x <= 0) * (elu(x) + ALPHA))\n",
        "\n",
        "# Leaky ReLU which at ALPHA = 1 becomes the identity function\n",
        "@jit\n",
        "def lrelu(x):\n",
        "  return ((x > 0) * x) + ((x <= 0) * x * ALPHA)\n",
        "\n",
        "@jit\n",
        "def d_lrelu(x):\n",
        "  return ((x > 0) * 1.0) + ((x <= 0) * ALPHA)\n",
        "\n",
        "# hyperbolic tan (clamped to the range [-10, 10] otherwise errors)\n",
        "@jit\n",
        "def tanh(x):\n",
        "  x = ((x > 10) * 10) + (((x >= -10) & (x <= 10)) * x) + ((x < -10) * -10)\n",
        "  e_pos = jnp.exp(x)\n",
        "  e_neg = jnp.exp(-x)\n",
        "  return (e_pos - e_neg) / (e_pos + e_neg)\n",
        "\n",
        "@jit\n",
        "def d_tanh(x):\n",
        "  return 1 - tanh(x)**2\n",
        "\n",
        "# Generic choice function\n",
        "@jit\n",
        "def activation(x, func):\n",
        "  return switch(func, [sigmoid, relu, elu, lrelu, tanh], x)\n",
        "@jit\n",
        "def d_activation(x, func):\n",
        "  return switch(func, [d_sigmoid, d_relu, d_elu, d_lrelu, d_tanh], x)\n",
        "        "
      ],
      "metadata": {
        "id": "S7uvLMF9CuGs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# cost functions and cost derivatives\n",
        "MEAN_SQUARED = 0\n",
        "MEAN_ABSOLUTE = 1\n",
        "MEAN_PERCENT = 2\n",
        "# MEAN_SQ_LOG = 3\n",
        "\n",
        "# mean squared\n",
        "@jit\n",
        "def mean_squared(actual_values, expected_values):\n",
        "  return (actual_values - expected_values)**2\n",
        "\n",
        "@jit\n",
        "def d_mean_squared(actual_values, expected_values):\n",
        "  return 2 * (actual_values - expected_values)\n",
        "\n",
        "# mean absolute\n",
        "@jit\n",
        "def mean_absolute(actual_values, expected_values):\n",
        "  return jnp.abs(actual_values - expected_values)\n",
        "\n",
        "@jit\n",
        "def d_mean_absolute(actual_values, expected_values):\n",
        "  x = actual_values - expected_values\n",
        "  return (x > 0) * 1.0 + (x < 0) * -1.0\n",
        "\n",
        "# mean percent\n",
        "@jit \n",
        "def mean_percent(actual_values, expected_values):\n",
        "  return (mean_absolute(actual_values, expected_values) / expected_values) * 100\n",
        "\n",
        "def d_mean_percent(actual_values, expected_values):\n",
        "  return 100 * d_mean_absolute(actual_values, expected_values)\n",
        "\n",
        "# @ jit\n",
        "# def mean_sq_log(actual_values, expected_values):\n",
        "#   return mean_squared(jnp.log(actual_values), jnp.log(expected_values))\n",
        "\n",
        "# @ jit\n",
        "# def d_mean_sq_log(actual_values, expected_values):\n",
        "#   return (2 / actual_values) * mean_squared(jnp.log(actual_values), jnp.log(expected_values))\n",
        "\n",
        "# Generic choice function\n",
        "@jit\n",
        "def error(actual_values, expected_values, func):\n",
        "  return switch(func, [mean_squared, mean_absolute, mean_percent], actual_values, expected_values)\n",
        "@jit\n",
        "def d_error(actual_values, expected_values, func):\n",
        "  return switch(func, [d_mean_squared, d_mean_absolute, d_mean_percent], actual_values, expected_values)"
      ],
      "metadata": {
        "id": "ULn21hx2DVRy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# layer creation functions\n",
        "Layer = namedtuple('Layer', 'weights biases activation error')\n",
        "\n",
        "# class Layer(NamedTuple):\n",
        "#   weights: jnp.array\n",
        "#   biases: jnp.array\n",
        "#   activation: int\n",
        "#   error: int\n",
        "\n",
        "# make layer\n",
        "# should return -> tuple containging a array(nodes in, nodes out) of weights and a array(nodes out) of biases\n",
        "@partial(jit, static_argnums=(1,))\n",
        "def make_layer(key, shape, activation, error):\n",
        "  key, subkey = random.split(key)\n",
        "  return key, Layer(random.uniform(key = subkey, minval = -10., maxval = 10., shape = (shape[0], shape[1])), jnp.zeros(shape = shape[1]), activation, error)"
      ],
      "metadata": {
        "id": "SEoDc15sESvm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# layer computation functions\n",
        "\n",
        "# def __call__(self, inputs):\n",
        "#         self._inputs = inputs\n",
        "#         for out_node in range(self._nodes_out):\n",
        "#             self._z_vals[out_node] = self._biases[out_node] + np.dot(inputs, self._weights[..., out_node])\n",
        "#             self._activation_vals[out_node] = self._activation.f(self._z_vals[out_node])\n",
        "#         return self._activation_vals\n",
        "@jit\n",
        "def calc_layer_output(layer, inputs):\n",
        "  z_vals = layer.biases + jnp.matmul(inputs, layer.weights)\n",
        "  outputs = activation(z_vals, layer.activation)\n",
        "  return z_vals, outputs\n",
        "\n",
        "# def _calc_output_node_vals(self, expected_output):\n",
        "#         node_values = np.zeros(shape = self._nodes_out)\n",
        "#         for out_node in range(self._nodes_out):\n",
        "#             cost_derivative = self._error.d_f(self._activation_vals[out_node], expected_output[out_node])\n",
        "#             activation_val_derivative = self._activation.d_f(self._z_vals[out_node])\n",
        "#             node_values[out_node] = cost_derivative * activation_val_derivative\n",
        "#         return node_values\n",
        "@jit\n",
        "def calc_output_node_vals(layer, z_vals, actual_output, expected_output):\n",
        "  return d_activation(z_vals, layer.activation) * d_error(actual_output, expected_output, layer.error)\n",
        "\n",
        "# def _calc_hidden_node_vals(self, old_layer, old_node_vals):\n",
        "#         new_node_vals = np.zeros(shape = self._nodes_out)\n",
        "#         for new_node in range(new_node_vals.shape[0]):\n",
        "#             new_node_val = 0\n",
        "#             for old_node in range(old_node_vals.shape[0]):\n",
        "#                 weighted_derivative = old_layer._weights[new_node, old_node]\n",
        "#                 new_node_val += weighted_derivative * old_node_vals[old_node]\n",
        "#             new_node_val *= self._activation.d_f(self._z_vals[new_node])\n",
        "#             new_node_vals[new_node] = new_node_val\n",
        "#         return new_node_vals\n",
        "@jit\n",
        "def calc_hidden_node_vals(layer, z_vals, node_vals, weights):\n",
        "  return d_activation(z_vals, layer.activation) * jnp.matmul(node_vals, jnp.transpose(weights))\n",
        "\n",
        "# def _update_gradients(self, node_values):\n",
        "#         for out_node in range(self._nodes_out):\n",
        "#             for in_node in range(self._nodes_in):\n",
        "#                 cost_weight_derivative = self._inputs[in_node] * node_values[out_node]\n",
        "#                 self._gradient_w[in_node, out_node] += cost_weight_derivative\n",
        "#             self._gradient_b[out_node] += node_values[out_node]\n",
        "@jit\n",
        "def update_gradient(gradient_w, gradient_b, inputs, node_values):\n",
        "  return gradient_w + jnp.matmul(inputs[:, None], node_values[None, :]), gradient_b + node_values\n",
        "\n",
        "# def _apply_gradients(self, learning_rate):\n",
        "#     self._biases -= (learning_rate * self._gradient_b)\n",
        "#     self._weights -= (learning_rate * self._gradient_w)\n",
        "@jit\n",
        "def apply_gradients(layer, gradient_w, gradient_b, learning_rate):\n",
        "  return Layer(layer.weights - (learning_rate * gradient_w), layer.biases - (learning_rate * gradient_b), layer.activation, layer.error)\n",
        "\n",
        "# def _clear_gradients(self):\n",
        "#     self._gradient_w = np.zeros(shape = (self._nodes_in, self._nodes_out))\n",
        "#     self._gradient_b = np.zeros(shape = self._nodes_out)\n",
        "@partial(jit, static_argnums=(0,))\n",
        "def reset_gradients(shape):\n",
        "  return jnp.zeros(shape = (shape[0], shape[1])), jnp.zeros(shape = shape[1])"
      ],
      "metadata": {
        "id": "6vLo7pa9dlfO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# neural network creation functions\n",
        "@partial(jit, static_argnums=(1,))\n",
        "def make_network(key, shape: tuple, activations: tuple, error: tuple):\n",
        "  layers = []\n",
        "  for i in range(len(shape) - 1):\n",
        "    key, layer = make_layer(key, shape = (shape[i], shape[i + 1]), activation = activations[i], error = error[i])\n",
        "    layers.append(layer)\n",
        "  return key, layers\n",
        "\n",
        "# def make_layer_list(i, val):\n",
        "#   key, shape, layers = val\n",
        "#   print(shape)\n",
        "#   key, layer = make_layer(key, shape = jnp.array([shape[i], shape[i + 1]]))\n",
        "#   layers.append(layer)\n",
        "#   return (key, shape, layers)\n",
        "  \n",
        "# # @partial(jit, static_argnums=(1,))\n",
        "# def make_network(key, shape):\n",
        "#   print(shape)\n",
        "#   result = fori_loop(0, len(shape) - 1, make_layer_list, (key, shape, []))\n",
        "#   return result[0], result[2]"
      ],
      "metadata": {
        "id": "zKbJL9EGpC18"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# neural network computation functions\n",
        "\n",
        "# def _calc_outputs(self, inputs):\n",
        "#         for layer in self._layers:\n",
        "#             inputs = layer(inputs)\n",
        "#         self._outputs = inputs\n",
        "def calc_network_output(network, inputs, learning = False):\n",
        "  z_vals_list = []\n",
        "  input_list = []\n",
        "  for layer in network:\n",
        "    input_list.append(inputs)\n",
        "    z_vals, inputs = calc_layer_output(layer, inputs)\n",
        "    z_vals_list.append(z_vals)\n",
        "  if learning:\n",
        "    return input_list, z_vals_list, inputs\n",
        "  return inputs\n",
        "\n",
        "@jit\n",
        "def classify(network, inputs):\n",
        "  return jnp.argmax(calc_network_output(network, inputs))\n",
        "\n",
        "# @ jit\n",
        "# def apply_prop(i, val):\n",
        "#   network, inputs, input_list, z_vals_list = val\n",
        "#   input_list.append(inputs)\n",
        "#   z_vals, inputs = calc_layer_output(network[i], inputs)\n",
        "#   z_vals_list.append(z_vals)\n",
        "#   return (network, inputs, input_list, z_vals_list)\n",
        "\n",
        "# @partial(jit, static_argnums = (2,))\n",
        "# def calc_network_output(network, inputs, learning = False):\n",
        "#   result = fori_loop(0, len(network), apply_prop, (network, inputs, [], []))\n",
        "#   if learning:\n",
        "#     return result[2], result[3], result[1]\n",
        "#   return result[1]\n",
        "\n",
        "# def _cost(self, data_point):\n",
        "#         self._calc_outputs(data_point['input'])\n",
        "#         output_layer = self._layers[len(self._layers) - 1]\n",
        "#         cost = 0.\n",
        "#         for out_node in range(self._outputs.shape[0]):\n",
        "#             cost += output_layer._error.f(self._outputs[out_node], data_point['expected_output'][out_node])\n",
        "#         return cost\n",
        "def cost(i, val):\n",
        "  total_cost, network, inputs, expected_outputs = val\n",
        "  return (total_cost + jnp.sum(error(calc_network_output(network, inputs[i]), expected_outputs[i], network[len(network) - 1].error)), network, inputs, expected_outputs)\n",
        "\n",
        "# def _avg_cost(self, data_points):\n",
        "#     total_cost = 0.\n",
        "#     for data_point in data_points:\n",
        "#         total_cost += self._cost(data_point)\n",
        "#     return total_cost / len(data_points)\n",
        "def avg_cost(network, inputs, expected_outputs):\n",
        "  total_cost, network, inputs, expected_outputs = fori_loop(0, len(inputs), cost, (0, network, inputs, expected_outputs))\n",
        "  return total_cost / len(inputs)\n",
        "\n",
        "def test(i, val):\n",
        "  total_correct, network, inputs, expected_outputs = val\n",
        "  total_correct += ((jnp.argmax(expected_outputs[i]) == jnp.argmax(calc_network_output(network, inputs[i]))) * 1)\n",
        "  return (total_correct, network, inputs, expected_outputs)\n",
        "\n",
        "def accuracy(network, inputs, expected_outputs):\n",
        "  total_correct, network, inputs, expected_output = fori_loop(0, len(inputs), test, (0, network, inputs, expected_outputs))\n",
        "  return total_correct / len(inputs)\n",
        "\n",
        "# def _apply_gradients(self, learning_rate):\n",
        "#       for layer in self._layers:\n",
        "#           layer._apply_gradients(learning_rate)\n",
        "def apply_all_gradients(network, gradient_w_list, gradient_b_list, learning_rate):\n",
        "  for layer in range(len(network)):\n",
        "    network[layer] = apply_gradients(network[layer], gradient_w_list[layer], gradient_b_list[layer], learning_rate)\n",
        "  return network\n",
        "\n",
        "# @ jit\n",
        "# def apply_per_layer(i, val):\n",
        "#   network, gradient_w_list, gradient_b_list, learning_rate = val\n",
        "#   network[i] = apply_gradients(network[i], gradient_w_list[i], gradient_b_list[i], learning_rate)\n",
        "#   return (network, gradient_w_list, gradient_b_list, learning_rate)\n",
        "\n",
        "# @ jit\n",
        "# def apply_all_gradients(network, gradient_w_list, gradient_b_list, learning_rate):\n",
        "#   result = fori_loop(0, len(network), apply_per_layer, (network, gradient_w_list, gradient_b_list, learning_rate))\n",
        "#   return result[0]\n",
        "\n",
        "# def _clear_gradients(self) :\n",
        "#     for layer in self._layers:\n",
        "#         layer._clear_gradients()\n",
        "def reset_gradient_list(network):\n",
        "  gradient_w_list = []\n",
        "  gradient_b_list = []\n",
        "  for layer in network:\n",
        "    gradient_w, gradient_b = reset_gradients(layer.weights.shape)\n",
        "    gradient_w_list.append(gradient_w)\n",
        "    gradient_b_list.append(gradient_b)\n",
        "  return gradient_w_list, gradient_b_list\n",
        "\n",
        "# @ jit\n",
        "# def create_gradient_list(i, val):\n",
        "#   network, gradient_w_list, gradient_b_list = val\n",
        "#   gradient_w, gradient_b = reset_gradients(network[i])\n",
        "#   gradient_w_list.append(gradient_w)\n",
        "#   gradient_b_list.append(gradient_b)\n",
        "#   return (network, gradient_w_list, gradient_b_list)\n",
        "\n",
        "# @ jit\n",
        "# def reset_gradient_list(network):\n",
        "#   result = fori_loop(0, len(network), create_gradient_list, (network, [], []))\n",
        "#   return result[1], result[2] \n",
        "\n",
        "# def _back_prop(self, data_point):\n",
        "#     self._calc_outputs(data_point['input'])\n",
        "#     output_layer = self._layers[len(self._layers) - 1]\n",
        "#     node_values = output_layer._calc_output_node_vals(data_point['expected_output'])\n",
        "#     output_layer._update_gradients(node_values)\n",
        "#     for layer in range(len(self._layers) - 2, -1, -1):\n",
        "#         hidden_layer = self._layers[layer]\n",
        "#         node_values = hidden_layer._calc_hidden_node_vals(self._layers[layer + 1], node_values)\n",
        "#         hidden_layer._update_gradients(node_values)\n",
        "def back_prop(network, gradient_w_list, gradient_b_list, inputs, expected_outputs):\n",
        "  input_list, z_vals_list, outputs = calc_network_output(network, inputs, learning = True)\n",
        "  node_values = calc_output_node_vals(network[len(network) - 1], z_vals_list[len(network) - 1], outputs, expected_outputs)\n",
        "  gradient_w_list[len(network) - 1], gradient_b_list[len(network) - 1] = update_gradient(gradient_w_list[len(network) - 1], gradient_b_list[len(network) - 1], input_list[len(network) - 1], node_values)\n",
        "  for layer in range(len(network) - 2, -1, -1):\n",
        "    node_values = calc_hidden_node_vals(network[layer], z_vals_list[layer], node_values, network[layer + 1].weights)\n",
        "    gradient_w_list[layer], gradient_b_list[layer] = update_gradient(gradient_w_list[layer], gradient_b_list[layer], input_list[layer], node_values)\n",
        "  return gradient_w_list, gradient_b_list\n",
        "\n",
        "# def update_all_gradients(i, val):\n",
        "#   input_list, z_vals_list, outputs, network, gradient_w_list, gradient_b_list = val\n",
        "#   node_values = cond(i == 1, calc_output_node_vals, calc_hidden_node_vals, network[len(network) - i], z_vals_list[len(network) - i])\n",
        "\n",
        "# @ jit\n",
        "# def back_prop(network, gradient_w_list, gradient_b_list, inputs, expected_outputs):\n",
        "#   input_list, z_vals_list, outputs = calc_network_output(network, inputs, learning = True)\n",
        "#   _, _, _, _, gradient_w_list, gradient_b_list = fori_loop(1, len(network) + 1, update_all_gradients, (input_list, z_vals_list, outputs, network, gradient_w_list, gradient_b_list))\n",
        "#   return gradient_w_list, gradient_b_list\n",
        "\n",
        "# def learn(self, batch, learning_rate):\n",
        "#         avg_vals = []\n",
        "#         for epoch in range(self._EPOCH):\n",
        "#             for pos in range(0, len(batch), self._BATCH_SIZE):\n",
        "#                 mini_batch = batch[pos : np.minimum(pos + self._BATCH_SIZE, len(batch))]\n",
        "#                 for data_point in mini_batch:\n",
        "#                     self._back_prop(data_point)\n",
        "#                 self._apply_gradients(learning_rate / np.minimum(self._BATCH_SIZE, len(mini_batch)))\n",
        "#                 self._clear_gradients()\n",
        "#                 # avg_vals.append(self._avg_cost(mini_batch))\n",
        "#             # self._apply_gradients(learning_rate / len(batch))\n",
        "#             # self._clear_gradients()\n",
        "#             avg_vals.append(self._avg_cost(batch))\n",
        "#         return avg_vals\n",
        "def apply_back_prop(i, val):\n",
        "  network, mini_batch, expected_outputs, gradient_w_list, gradient_b_list = val\n",
        "  gradient_w_list, gradient_b_list = back_prop(network, gradient_w_list, gradient_b_list, mini_batch[i], expected_outputs[i])\n",
        "  return (network, mini_batch, expected_outputs, gradient_w_list, gradient_b_list)\n",
        "\n",
        "def learn(network, mini_batch, mini_expected, learning_rate):\n",
        "  gradient_w_list, gradient_b_list = reset_gradient_list(network)\n",
        "  network, mini_batch,  mini_expected, gradient_w_list, gradient_b_list = fori_loop(0, len(mini_batch), apply_back_prop, (network, mini_batch,  mini_expected, gradient_w_list, gradient_b_list))\n",
        "  return apply_all_gradients(network, gradient_w_list, gradient_b_list, learning_rate / len(mini_batch))\n",
        "\n",
        "def train(network, batch, expected_outputs, # training information\n",
        "          learning_rate = .001, epochs = 10, batch_size = 1000, # meta parameters\n",
        "          train_percent = 1, set_aside_training_data = True, test_batch = None, test_outputs = None): # testing information\n",
        "  all_loss = []\n",
        "  all_acc = []\n",
        "\n",
        "  if set_aside_training_data:\n",
        "    train_batch = batch[0 : int(jnp.floor(len(batch) * train_percent))]\n",
        "    train_outputs = expected_outputs[0 : int(jnp.floor(len(expected_outputs) * train_percent))]\n",
        "    test_batch = batch[int(jnp.floor(len(batch) * train_percent)) : len(batch)]\n",
        "    test_outputs = expected_outputs[int(jnp.floor(len(expected_outputs) * train_percent)) : len(expected_outputs)]\n",
        "  elif test_batch == None or test_outputs == None:\n",
        "    print(\"No testing data, but no training data was set aside either (if you want no data to be set aside, use train_percent = 1 instead).\")\n",
        "    return all_loss, all_acc, network\n",
        "  else:\n",
        "    train_batch = batch\n",
        "    train_outputs = expected_outputs\n",
        "\n",
        "\n",
        "  for epoch in range(epochs):\n",
        "    for pos in range(0, len(train_batch), batch_size):\n",
        "      mini_batch = train_batch[pos : jnp.minimum(pos + batch_size, len(train_batch))]\n",
        "      mini_outputs = train_outputs[pos : jnp.minimum(pos + batch_size, len(train_batch))]\n",
        "\n",
        "      network = learn(network, mini_batch, mini_outputs, learning_rate)\n",
        "\n",
        "      all_loss.append(avg_cost(network, mini_batch, mini_outputs))\n",
        "      # all_acc.append(test(network, mini_batch, mini_expected))\n",
        "    # all_avg.append(avg_cost(network, batch, expected_outputs))\n",
        "    all_acc.append(accuracy(network, test_batch, test_outputs))\n",
        "  return all_acc, all_loss, network\n"
      ],
      "metadata": {
        "id": "u_a5GcWP_rRr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def deconstruct(network):\n",
        "  x, y = 0, 0\n",
        "  for layer in network:\n",
        "    x += layer.weights.shape[0]\n",
        "    y += layer.weights.shape[1]\n",
        "\n",
        "  weights = [[0 for _ in range(y)] for _ in range(x)]\n",
        "  biases = [0 for _ in range(y)]\n",
        "  activations = [0 for _ in range(len(network))] \n",
        "  errors = [0 for _ in range(len(network))]\n",
        "\n",
        "  offset_x, offset_y = 0, 0\n",
        "  for l, layer in enumerate(network):\n",
        "    for i in range(layer.weights.shape[0]):\n",
        "      for j in range(layer.weights.shape[1]):\n",
        "        weights[i + offset_x][j + offset_y] = layer.weights[i][j]\n",
        "    offset_x += layer.weights.shape[0]\n",
        "\n",
        "    for j in range(layer.weights.shape[1]):\n",
        "      biases[j + offset_y] = layer.biases[j]\n",
        "    offset_y += layer.weights.shape[1]\n",
        "\n",
        "    activations[l] = layer.activation\n",
        "    errors[l] = layer.error\n",
        "\n",
        "  return weights, biases, activations, errors\n",
        "\n",
        "def replicate(network, count):\n",
        "  weights, biases, activations, errors = deconstruct(network)\n",
        "  rep = lambda x: jnp.array([x] * count)\n",
        "  return rep(weights), rep(biases), rep(activations), rep(errors)\n",
        "\n",
        "def split(arr, count):\n",
        "  return arr.reshape(count, arr.shape[0] // count, *arr.shape[1:])\n",
        "\n",
        "def reconstruct(network, weights, biases, activations, errors):\n",
        "  pass"
      ],
      "metadata": {
        "id": "3jcVDX-vE5V4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def p_reset_gradient_list(weights, biases):\n",
        "  return jnp.zeros(shape = (weights.shape[0], weights.shape[1])), jnp.zeros(shape = biases.shape[0])\n",
        "\n",
        "def p_back_prop(weights, biases, activations, errors, gradient_w, gradient_b, input, expected_output):\n",
        "  input_list, z_vals_list, outputs = p_calc_network_output(network, inputs, learning = True)\n",
        "  node_values = p_calc_output_node_vals(network[len(network) - 1], z_vals_list[len(network) - 1], outputs, expected_outputs)\n",
        "  gradient_w_list[len(network) - 1], gradient_b_list[len(network) - 1] = p_update_gradient(gradient_w_list[len(network) - 1], gradient_b_list[len(network) - 1], input_list[len(network) - 1], node_values)\n",
        "  for layer in range(len(network) - 2, -1, -1):\n",
        "    node_values = calc_hidden_node_vals(network[layer], z_vals_list[layer], node_values, network[layer + 1].weights)\n",
        "    gradient_w_list[layer], gradient_b_list[layer] = update_gradient(gradient_w_list[layer], gradient_b_list[layer], input_list[layer], node_values)\n",
        "  return gradient_w_list, gradient_b_list\n",
        "\n",
        "def p_apply_back_prop(i, val):\n",
        "  weights, biases, activations, errors, mini_batch, mini_expected, gradient_w, gradient_b = val\n",
        "  gradient_w, gradient_b = p_back_prop(weights, biases, activations, errors, gradient_w, gradient_b, mini_batch[i], mini_expected[i])\n",
        "  return (weights, biases, activations, errors, mini_batch, mini_expected, gradient_w, gradient_b)\n",
        "\n",
        "def p_apply_gradients(weights, biases, gradient_w, gradient_b, learning_rate):\n",
        "  pass\n",
        "\n",
        "# parallelized training functions\n",
        "@partial(pmap, axis_name=\"mini_batch\")\n",
        "def p_learn(weights, biases, activations, errors, mini_batch, mini_expected, learning_rate):\n",
        "  gradient_w, gradient_b = p_reset_gradient_list(weights, biases)\n",
        "  network, mini_batch, mini_expected, gradient_w_list, gradient_b_list = fori_loop(0, len(mini_batch), p_apply_back_prop, (weights, biases, activations, errors, mini_batch, mini_expected, gradient_w, gradient_b))\n",
        "\n",
        "  gradient_w = pmean(gradient_w, axis_name=\"mini_batch\")\n",
        "  gradient_b = pmean(gradient_b, axis_name=\"mini_batch\")\n",
        "\n",
        "  return p_apply_gradients(weights, biases, gradient_w, gradient_b, learning_rate / len(mini_batch))\n",
        "\n",
        "def p_train(network, batch, expected_outputs, # training information\n",
        "          learning_rate = .001, epochs = 10, batch_size = 1000, # meta parameters\n",
        "          train_percent = 1, set_aside_training_data = True, test_batch = None, test_outputs = None, # testing information\n",
        "          device_count = 1): # device information\n",
        "  all_loss = []\n",
        "  all_acc = []\n",
        "\n",
        "  if set_aside_training_data:\n",
        "    train_batch = batch[0 : int(jnp.floor(len(batch) * train_percent))]\n",
        "    train_outputs = expected_outputs[0 : int(jnp.floor(len(expected_outputs) * train_percent))]\n",
        "    test_batch = batch[int(jnp.floor(len(batch) * train_percent)) : len(batch)]\n",
        "    test_outputs = expected_outputs[int(jnp.floor(len(expected_outputs) * train_percent)) : len(expected_outputs)]\n",
        "  elif test_batch == None or test_outputs == None:\n",
        "    print(\"No testing data, but no training data was set aside either (if you want no data to be set aside, use train_percent = 1 instead).\")\n",
        "    return all_loss, all_acc, network\n",
        "  else:\n",
        "    train_batch = batch\n",
        "    train_outputs = expected_outputs\n",
        "\n",
        "  replica_w, replica_b, replica_a, replica_e = replicate(network, device_count)\n",
        "\n",
        "  for epoch in range(epochs):\n",
        "    for pos in range(0, len(train_batch), batch_size * device_count):\n",
        "      mini_batch_split = split(train_batch[pos : jnp.minimum(pos + batch_size * device_count, len(train_batch))], device_count)\n",
        "      mini_outputs_split = split(train_outputs[pos : jnp.minimum(pos + batch_size * device_count, len(train_batch))], device_count)\n",
        "\n",
        "      replica_w, replica_b, replica_a, replica_e = p_learn(replica_w, replica_b, replica_a, replica_e, mini_batch_split, mini_outputs_split, learning_rate)\n",
        "\n",
        "    network = reconstruct(network, replica_w[0], replica_b[0], replica_a[0], replica_e[0])\n",
        "    all_loss.append(avg_cost(network, batch, expected_outputs))\n",
        "    all_acc.append(accuracy(network, test_batch, test_outputs))\n",
        "\n",
        "  return all_acc, all_loss, network"
      ],
      "metadata": {
        "id": "S6tn2MxC33RL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_google_mnist(file):\n",
        "  with open(file, newline = '') as csv_file:\n",
        "    reader = csv.reader(csv_file, delimiter = ',', quotechar = '\"')\n",
        "    \n",
        "    y_list = []\n",
        "    x_list = []\n",
        "\n",
        "    for row in reader:\n",
        "      y = jnp.zeros(shape = 10)\n",
        "      y = y.at[int(row[0])].set(1)\n",
        "\n",
        "      y_list.append(y)\n",
        "      x_list.append(jnp.array([int(x) for x in row[1 :]]) / 255)\n",
        "\n",
        "    return jnp.array(y_list), jnp.array(x_list)"
      ],
      "metadata": {
        "id": "Za4mK_-U6XO8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_output, training_data = preprocess_google_mnist(\"./sample_data/mnist_train_small.csv\")\n",
        "testing_output, testing_data = preprocess_google_mnist(\"./sample_data/mnist_test.csv\")"
      ],
      "metadata": {
        "id": "Ae9I1X0c9DuQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "key, net = make_network(key, shape = (784, 100, 10), activations = (RELU, SIGMOID), error = (MEAN_SQUARED, MEAN_SQUARED))"
      ],
      "metadata": {
        "id": "6By87CsDEqxe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "devices()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q1vsiitL-JLc",
        "outputId": "ed40ef03-c4bd-4cec-a364-204f0d14a7f1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0),\n",
              " TpuDevice(id=1, process_index=0, coords=(0,0,0), core_on_chip=1),\n",
              " TpuDevice(id=2, process_index=0, coords=(1,0,0), core_on_chip=0),\n",
              " TpuDevice(id=3, process_index=0, coords=(1,0,0), core_on_chip=1),\n",
              " TpuDevice(id=4, process_index=0, coords=(0,1,0), core_on_chip=0),\n",
              " TpuDevice(id=5, process_index=0, coords=(0,1,0), core_on_chip=1),\n",
              " TpuDevice(id=6, process_index=0, coords=(1,1,0), core_on_chip=0),\n",
              " TpuDevice(id=7, process_index=0, coords=(1,1,0), core_on_chip=1)]"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "numbert = net\n",
        "\n",
        "accuracy_list, loss_list, numbert = train(numbert, training_data, training_output, learning_rate = .3, epochs = 9000, batch_size = 100,\n",
        "                                                 set_aside_training_data = False, test_batch = testing_data, test_outputs = testing_output)\n",
        "\n",
        "plt.figure(figsize=(9, 9), dpi=80)\n",
        "\n",
        "plt.subplot(2, 1, 1)\n",
        "plt.plot(accuracy_list)\n",
        "plt.title(\"Numbert's Accuracy\")\n",
        "plt.ylim(0, 1)\n",
        "\n",
        "plt.subplot(2, 1, 2)\n",
        "plt.plot(loss_list)\n",
        "plt.title(\"Numbert's Loss\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Bbcl46O7wagr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "numbert = net\n",
        "# print(deconstruct(numbert))\n",
        "\n",
        "accuracy_list, loss_list, numbert = train(numbert, training_data, training_output, learning_rate = .05, epochs = 3000, batch_size = 100,\n",
        "                                                 set_aside_training_data = False, test_batch = testing_data, test_outputs = testing_output)\n",
        "\n",
        "plt.figure(figsize=(9, 9), dpi=80)\n",
        "\n",
        "plt.subplot(2, 1, 1)\n",
        "plt.plot(accuracy_list)\n",
        "plt.title(\"Numbert's Accuracy\")\n",
        "plt.ylim(0, 1)\n",
        "\n",
        "plt.subplot(2, 1, 2)\n",
        "plt.plot(loss_list)\n",
        "plt.title(\"Numbert's Loss\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "PjsoRvN0D1Ok"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "numbert = net\n",
        "# print(deconstruct(numbert))\n",
        "\n",
        "accuracy_list, loss_list, numbert = train(numbert, training_data, training_output, learning_rate = .3, epochs = 3000, batch_size = 50,\n",
        "                                                 set_aside_training_data = False, test_batch = testing_data, test_outputs = testing_output)\n",
        "\n",
        "plt.figure(figsize=(9, 9), dpi=80)\n",
        "\n",
        "plt.subplot(2, 1, 1)\n",
        "plt.plot(accuracy_list)\n",
        "plt.title(\"Numbert's Accuracy\")\n",
        "plt.ylim(0, 1)\n",
        "\n",
        "plt.subplot(2, 1, 2)\n",
        "plt.plot(loss_list)\n",
        "plt.title(\"Numbert's Loss\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "gNUuUZ1kwqSi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "numbert = net\n",
        "# print(deconstruct(numbert))\n",
        "\n",
        "accuracy_list, loss_list, numbert = train(numbert, training_data, training_output, learning_rate = .05, epochs = 3000, batch_size = 50,\n",
        "                                                 set_aside_training_data = False, test_batch = testing_data, test_outputs = testing_output)\n",
        "\n",
        "plt.figure(figsize=(9, 9), dpi=80)\n",
        "\n",
        "plt.subplot(2, 1, 1)\n",
        "plt.plot(accuracy_list)\n",
        "plt.title(\"Numbert's Accuracy\")\n",
        "plt.ylim(0, 1)\n",
        "\n",
        "plt.subplot(2, 1, 2)\n",
        "plt.plot(loss_list)\n",
        "plt.title(\"Numbert's Loss\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ErU90LKlwnCg"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}